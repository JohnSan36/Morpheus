{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())  \n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "response = llm.invoke(\"Olá Gemini\") \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import redis\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "#-------------------------------------------------Functions---------------------------------------------------------------\n",
    "\n",
    "def obter_hora_e_data_atual():\n",
    "    agora = datetime.now()\n",
    "    return agora.strftime(\"%d-%m-%Y - %H:%M:%S\")\n",
    "\n",
    "def get_memory_for_user(whatsapp):\n",
    "    memory = RedisChatMessageHistory(\n",
    "        session_id=whatsapp, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"memory\", chat_memory=memory)\n",
    "\n",
    "def get_memory_for_user_gemini(session_id_key: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\", chat_memory=chat_memory)\n",
    "\n",
    "\n",
    "#------------------------------------------------Webhooks-------------------------------------------------------------------\n",
    "\n",
    "user_id = \"John Santos\"\n",
    "user_number = \"5541996143338\"\n",
    "data_atual = obter_hora_e_data_atual()\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------Tools----------------------------------------------------------\n",
    "    \n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    responsta = requests.post(url, headers=headers, json=payload)\n",
    "    responsta.raise_for_status()\n",
    "    return responsta.json()\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memoria da conversa quando solicitado\"\"\"\n",
    "    r = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "    session_prefix = f\"message_store:{user_id}\"\n",
    "    chaves = r.keys(session_prefix + \"*\")\n",
    "    for chave in chaves:\n",
    "        r.delete(chave)\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "memoria_gemini = get_memory_for_user_gemini(user_id)\n",
    "memoria = get_memory_for_user(user_id)\n",
    "\n",
    "#--------------------------------------------------Tools----------------------------------------------------------\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"\"\"\n",
    "        Você é Morpheus, um assistente disruptivo. A data atual é {data_atual}.\n",
    "        Não use asteriscos, listas devem começar com '-'.\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "prompt_gemini = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é Morpheus, um assistente disruptivo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=api_key, temperature=0.0)\n",
    "    tools_json = [convert_to_openai_function(t) for t in tools]\n",
    "    pass_through = RunnablePassthrough.assign(agent_scratchpad=lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"]))\n",
    "    chain = pass_through | prompt | llm.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=memoria,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True\n",
    "    )\n",
    "\n",
    "elif LLM_PROVIDER == \"gemini\":\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-preview-05-06\", google_api_key=GOOGLE_API_KEY, temperature=0.0)\n",
    "    chain = create_tool_calling_agent(llm, tools, prompt_gemini)\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=memoria_gemini,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True, \n",
    "        handle_parsing_errors=True  \n",
    "    )\n",
    "\n",
    "\n",
    "resposta = agent_executor.invoke({\"input\": \"qual foi a ultima mensagem que lhe enviei?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtional selection of Gpt an Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# ------------------------------------------------- Functions ---------------------------------------------------------------\n",
    "\n",
    "def obter_hora_e_data_atual():\n",
    "    agora = datetime.now()\n",
    "    return agora.strftime(\"%d-%m-%Y - %H:%M:%S\")\n",
    "\n",
    "def get_memory(session_id_key: str, memory_key_name: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=memory_key_name, chat_memory=chat_memory, output_key='output')\n",
    "\n",
    "# ------------------------------------------------ Webhooks -------------------------------------------------------------------\n",
    "\n",
    "user_id = \"John_Santos\"\n",
    "user_number = \"5541996143338\"\n",
    "data_atual = obter_hora_e_data_atual()\n",
    "\n",
    "# -------------------------------------------------- Tools ----------------------------------------------------------\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": user_number,\n",
    "        \"text\": texto,\n",
    "        \"options\": {\"delay\": 800}\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Erro na requisição para Evolution API: {e}\"\n",
    "    \n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memoria da conversa atual (session_id) do Redis.\"\"\"\n",
    "    chat_memory_to_clear = RedisChatMessageHistory(\n",
    "        session_id=user_id,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    chat_memory_to_clear.clear()\n",
    "    return f\"Memória para a sessão '{user_id}' foi excluída com sucesso.\"\n",
    "\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "\n",
    "# -------------------------------------------------- Prompts ----------------------------------------------------------\n",
    "\n",
    "\n",
    "prompt_openai_format = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"Você é Morpheus (OpenAI). Data: {data_atual}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_gemini_format = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        f\"Você é Morpheus (Gemini), um assistente que mantém o contexto da conversa.\"\n",
    "        \"**Instruções para o uso de ferramentas:**\\n\"\n",
    "        \"- **Para enviar uma mensagem**: Se o usuário solicitar 'enviar uma mensagem' ou 'enviar um ', utilize a ferramenta\\n\"\n",
    "        \"- **Para excluir a memória**: Se o usuário pedir para 'excluir a memória', 'limpar a conversa' ou frases similares, utilize a ferramenta `excluir_memoria`.\\n\\n\"\n",
    "        \"Responda de forma útil e concisa, usando as ferramentas quando apropriado.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "\n",
    "    active_memory = get_memory(user_id, \"memory\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=OPENAI_API_KEY, temperature=0.0)\n",
    "    tools_json = [convert_to_openai_function(t) for t in tools]\n",
    "    chain = RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"])) | prompt_openai_format | llm.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=active_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "\n",
    "elif LLM_PROVIDER == \"gemini\":\n",
    "\n",
    "    active_memory = get_memory(user_id, \"chat_history\")\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", google_api_key=GOOGLE_API_KEY, temperature=0.5) \n",
    "    chain = create_tool_calling_agent(llm, tools, prompt_gemini_format) \n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=active_memory, \n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "input_1 = \"Envie uma mensagem para mim dizendo oi?.\"\n",
    "resposta_1 = agent_executor.invoke({\"input\": input_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool # Adicionado o import que faltava na sua última versão\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "import redis\n",
    "import os\n",
    "import requests \n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "user_number = \"5541999999999\" \n",
    "\n",
    "def get_memory_for_user(session_id_key: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\", chat_memory=chat_memory)\n",
    "\n",
    "user_id = \"John_Santos_Test_Session\"\n",
    "memoria = get_memory_for_user(user_id)\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada para o WhatsApp do usuário.\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    try:\n",
    "        resposta = requests.post(url, headers=headers, json=payload)\n",
    "        resposta.raise_for_status()\n",
    "        return resposta.json()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao enviar mensagem: {e}\"\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memória da conversa atual  do Redis.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(REDIS_URL)\n",
    "        num_deleted = r.delete(user_id)\n",
    "        return f\"Memória para a sessão '{user_id}' foi excluída com sucesso.\" if num_deleted > 0 else f\"Nenhuma memória encontrada para a sessão '{user_id}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao tentar excluir memória: {e}\"\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é Morpheus, um assistente disruptivo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.0)\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=memoria,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True, \n",
    "    handle_parsing_errors=True  \n",
    ")\n",
    "\n",
    "primeira_input = \"Envie uma mensagem para mim dizendo que bonito\"\n",
    "resposta1 = agent_executor.invoke({\"input\": primeira_input})\n",
    "print(resposta1.get(\"output\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada para o WhatsApp do usuário.\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    try:\n",
    "        resposta = requests.post(url, headers=headers, json=payload)\n",
    "        resposta.raise_for_status()\n",
    "        return resposta.json()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao enviar mensagem: {e}\"\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memória da conversa atual  do Redis.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(REDIS_URL)\n",
    "        num_deleted = r.delete(user_id)\n",
    "        return f\"Memória para a sessão '{user_id}' foi excluída com sucesso.\" if num_deleted > 0 else f\"Nenhuma memória encontrada para a sessão '{user_id}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao tentar excluir memória: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "\n",
    "query = \"eNVIE UMA MENSAGEM  de whatsapp mandando um oi\"\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(ai_msg)\n",
    "\n",
    "messages.append(ai_msg)\n",
    "\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"enviar_msg\": enviar_msg, \"excluir_memoria\": excluir_memoria}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages\n",
    "\n",
    "llm_with_tools.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "model = \"google/gemini-2.5-pro-preview\"\n",
    "\n",
    "\n",
    "def invoke(message):\n",
    "    response = requests.post(\n",
    "\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": \"Bearer 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\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"model\": model, \n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": message\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                    \"url\": \"\"\n",
    "                    }\n",
    "                }]\n",
    "            }],\n",
    "        })\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "invoke(\"quem é julio cesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from agent.tools import soma\n",
    "import os\n",
    "\n",
    "REDIS_URL = \"redis://default:QR02Eq2QKPlH6pJMN4wUo3XuEKUfqrAz@redis-16516.crce196.sa-east-1-2.ec2.redns.redis-cloud.com:16516\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "def get_memory_for_user(session_id):\n",
    "    history = RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key='memoria', chat_memory=history)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Você é um assistente útil que usa as tools conforme necessidade.\",\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name='memoria'),\n",
    "    ('human', '{input}'),\n",
    "    MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "])\n",
    "\n",
    "#parcial_prompt = prompt.partial(\n",
    "#    input_language=\"portugues\",\n",
    "#    output_language=\"japones\"\n",
    "#)\n",
    "\n",
    "tools = [soma]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "active_memory = get_memory_for_user('session_id_placholder') \n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=active_memory, \n",
    "    return_intermediate_steps=True,\n",
    "    tools=tools,  \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    'input': 'Bom dia',\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# As mensagens têm o tipo \"lista\". A função `add_messages` - na anotação define como \n",
    "# essa chave de estado deve ser atualizada - (neste caso, ela adiciona mensagens à \n",
    "# lista, em vez de sobrescrevê-las)\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "class State(TypedDict):\n",
    "   \n",
    "    messages: Annotated[list, add_messages]  \n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# -------------------------ADICIONANDO UM NÓ-----------------------\n",
    "# ------------Importando o modelo------------\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "# ----------Incorporando-o em um nó----------\n",
    "# O primeiro argumento é o nome único do nó\n",
    "# O segundo argumento é a função ou objeto \n",
    "# que será chamado sempre que o nó for utilizado.\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# -------------------------Desenhando o fluxo-----------------------\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executando com Stram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------Executando o agente-----------------------\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Adeus!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        print(\"Erro, nenhuma mensagem encontrada \")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executando com Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = \"Qual doi minha ultima msg?\"\n",
    "resposta_graph = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": entrada}]})\n",
    "print(resposta_graph[\"messages\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "tool.invoke(\"O que é um 'nó' no LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Olá! Como posso ajudar você hoje?\n",
      "Assistant: Olá! Como posso ajudar você hoje?\n",
      "Assistant: \n",
      "Assistant: {\"query\": \"como criar um grafo\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://graphonline.top/pt/create_graph_by_matrix\", \"title\": \"Criando grafo a partir de uma matriz de adjac\\u00eancia - Graph Online\", \"content\": \"Criando um grafo a partir de uma matriz de adjac\\u00eancia. Nesta p\\u00e1gina, voc\\u00ea pode criar uma matriz de adjac\\u00eancia e desenhar o grafo.\", \"score\": 0.60694176, \"raw_content\": null}, {\"url\": \"https://medium.com/creditas-tech/gerando-grafos-no-google-sheets-f1d794a90394\", \"title\": \"Gerando Grafos no Google Sheets | Creditas Tech - Medium\", \"content\": \"Um grafo \\u00e9 um par (V, A) em que V \\u00e9 um conjunto arbitr\\u00e1rio e A \\u00e9 um subconjunto de V\\u00b2, sendo V\\u00b2 o conjunto de todos os pares n\\u00e3o ordenados de\", \"score\": 0.57408494, \"raw_content\": null}], \"response_time\": 3.04}\n",
      "Assistant: Aqui estão algumas fontes que te ensinam como criar um grafo:\n",
      "\n",
      "1. **Criando grafo a partir de uma matriz de adjacência** - [Graph Online](https://graphonline.top/pt/create_graph_by_matrix): Nesta página, você pode criar uma matriz de adjacência e desenhar o grafo.\n",
      "\n",
      "2. **Gerando Grafos no Google Sheets** - [Creditas Tech - Medium](https://medium.com/creditas-tech/gerando-grafos-no-google-sheets-f1d794a90394): Este artigo explica sobre grafos e como gerá-los usando o Google Sheets.\n",
      "\n",
      "Esses recursos devem te ajudar a entender como criar e manipular grafos!\n",
      "Assistant: Hello! How can I assist you today?\n",
      "Assistant: Sinto muito por não ter ajudado como você esperava. Como posso melhorar ou o que você gostaria de saber? Estou aqui para ajudar!\n",
      "Assistant: \n",
      "Assistant: {\"query\": \"\\u00faltima mensagem\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.letras.mus.br/banda-calypso/ultima-mensagem/\", \"title\": \"\\u00daltima Mensagem - Banda Calypso - LETRAS.MUS.BR\", \"content\": \"Estou te enviando a \\u00faltima mensagem. Depois de tanto tempo, arranjei coragem. Pra te dizer que o nosso caso acabou. J\\u00e1 n\\u00e3o suporto mais as dores desse amor.\", \"score\": 0.72884524, \"raw_content\": null}], \"response_time\": 2.91}\n",
      "Assistant: A última mensagem a que você pode se referir pode ser uma letra da canção \"Última Mensagem\" da Banda Calypso. A música fala sobre o fim de um relacionamento e inclui a frase: \"Estou te enviando a última mensagem. Depois de tanto tempo, arranjei coragem. Pra te dizer que o nosso caso acabou.\" \n",
      "\n",
      "Para ler a letra completa, você pode visitar [este link](https://www.letras.mus.br/banda-calypso/ultima-mensagem/). \n",
      "\n",
      "Se você estava se referindo a algo diferente, por favor, me dê mais detalhes!\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  \n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "llm_with_tools = llm.bind_tools(tools) # Modification: tell the LLM which tools it can call - highlight-next-line\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"Um nó que executa as ferramentas solicitadas na última AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! Como é meu nome?.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Olá! Não sei o seu nome. Você poderia me dizer?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "user_input = \"Hi there! Como é meu nome?.\"\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there! Como Vai.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Olá! Estou bem, obrigado! E você, como está?\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "user_input = \"Hi there! Como Vai.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_ns\": \"default\", \"checkpoint_id\": \"default\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
