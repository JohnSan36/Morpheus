{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())  \n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "response = llm.invoke(\"OlÃ¡ Gemini\") \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import redis\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "#-------------------------------------------------Functions---------------------------------------------------------------\n",
    "\n",
    "def obter_hora_e_data_atual():\n",
    "    agora = datetime.now()\n",
    "    return agora.strftime(\"%d-%m-%Y - %H:%M:%S\")\n",
    "\n",
    "def get_memory_for_user(whatsapp):\n",
    "    memory = RedisChatMessageHistory(\n",
    "        session_id=whatsapp, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"memory\", chat_memory=memory)\n",
    "\n",
    "def get_memory_for_user_gemini(session_id_key: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\", chat_memory=chat_memory)\n",
    "\n",
    "\n",
    "#------------------------------------------------Webhooks-------------------------------------------------------------------\n",
    "\n",
    "user_id = \"John Santos\"\n",
    "user_number = \"5541996143338\"\n",
    "data_atual = obter_hora_e_data_atual()\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------Tools----------------------------------------------------------\n",
    "    \n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    responsta = requests.post(url, headers=headers, json=payload)\n",
    "    responsta.raise_for_status()\n",
    "    return responsta.json()\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memoria da conversa quando solicitado\"\"\"\n",
    "    r = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "    session_prefix = f\"message_store:{user_id}\"\n",
    "    chaves = r.keys(session_prefix + \"*\")\n",
    "    for chave in chaves:\n",
    "        r.delete(chave)\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "memoria_gemini = get_memory_for_user_gemini(user_id)\n",
    "memoria = get_memory_for_user(user_id)\n",
    "\n",
    "#--------------------------------------------------Tools----------------------------------------------------------\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"\"\"\n",
    "        VocÃª Ã© Morpheus, um assistente disruptivo. A data atual Ã© {data_atual}.\n",
    "        NÃ£o use asteriscos, listas devem comeÃ§ar com '-'.\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "prompt_gemini = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© Morpheus, um assistente disruptivo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=api_key, temperature=0.0)\n",
    "    tools_json = [convert_to_openai_function(t) for t in tools]\n",
    "    pass_through = RunnablePassthrough.assign(agent_scratchpad=lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"]))\n",
    "    chain = pass_through | prompt | llm.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=memoria,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True\n",
    "    )\n",
    "\n",
    "elif LLM_PROVIDER == \"gemini\":\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-preview-05-06\", google_api_key=GOOGLE_API_KEY, temperature=0.0)\n",
    "    chain = create_tool_calling_agent(llm, tools, prompt_gemini)\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=memoria_gemini,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True, \n",
    "        handle_parsing_errors=True  \n",
    "    )\n",
    "\n",
    "\n",
    "resposta = agent_executor.invoke({\"input\": \"qual foi a ultima mensagem que lhe enviei?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtional selection of Gpt an Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `enviar_msg` with `{'texto': 'oi'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'key': {'remoteJid': '554196143338@s.whatsapp.net', 'fromMe': True, 'id': '3EB0B2AA82D8DEFF0F5F5780A3C75C13A03B1B22'}, 'pushName': '', 'status': 'PENDING', 'message': {'conversation': 'oi'}, 'contextInfo': None, 'messageType': 'conversation', 'messageTimestamp': 1748007964, 'instanceId': '40d11540-3974-449a-967f-2b7a28d621b3', 'source': 'unknown'}\u001b[0m\u001b[32;1m\u001b[1;3mEnviei uma mensagem para vocÃª dizendo \"oi\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# ------------------------------------------------- Functions ---------------------------------------------------------------\n",
    "\n",
    "def obter_hora_e_data_atual():\n",
    "    agora = datetime.now()\n",
    "    return agora.strftime(\"%d-%m-%Y - %H:%M:%S\")\n",
    "\n",
    "def get_memory(session_id_key: str, memory_key_name: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=memory_key_name, chat_memory=chat_memory, output_key='output')\n",
    "\n",
    "# ------------------------------------------------ Webhooks -------------------------------------------------------------------\n",
    "\n",
    "user_id = \"John_Santos\"\n",
    "user_number = \"5541996143338\"\n",
    "data_atual = obter_hora_e_data_atual()\n",
    "\n",
    "# -------------------------------------------------- Tools ----------------------------------------------------------\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": user_number,\n",
    "        \"text\": texto,\n",
    "        \"options\": {\"delay\": 800}\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Erro na requisiÃ§Ã£o para Evolution API: {e}\"\n",
    "    \n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memoria da conversa atual (session_id) do Redis.\"\"\"\n",
    "    chat_memory_to_clear = RedisChatMessageHistory(\n",
    "        session_id=user_id,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    chat_memory_to_clear.clear()\n",
    "    return f\"MemÃ³ria para a sessÃ£o '{user_id}' foi excluÃ­da com sucesso.\"\n",
    "\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "\n",
    "# -------------------------------------------------- Prompts ----------------------------------------------------------\n",
    "\n",
    "\n",
    "prompt_openai_format = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"VocÃª Ã© Morpheus (OpenAI). Data: {data_atual}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_gemini_format = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        f\"VocÃª Ã© Morpheus (Gemini), um assistente que mantÃ©m o contexto da conversa.\"\n",
    "        \"**InstruÃ§Ãµes para o uso de ferramentas:**\\n\"\n",
    "        \"- **Para enviar uma mensagem**: Se o usuÃ¡rio solicitar 'enviar uma mensagem' ou 'enviar um ', utilize a ferramenta\\n\"\n",
    "        \"- **Para excluir a memÃ³ria**: Se o usuÃ¡rio pedir para 'excluir a memÃ³ria', 'limpar a conversa' ou frases similares, utilize a ferramenta `excluir_memoria`.\\n\\n\"\n",
    "        \"Responda de forma Ãºtil e concisa, usando as ferramentas quando apropriado.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "\n",
    "    active_memory = get_memory(user_id, \"memory\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=OPENAI_API_KEY, temperature=0.0)\n",
    "    tools_json = [convert_to_openai_function(t) for t in tools]\n",
    "    chain = RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"])) | prompt_openai_format | llm.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=active_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "\n",
    "elif LLM_PROVIDER == \"gemini\":\n",
    "\n",
    "    active_memory = get_memory(user_id, \"chat_history\")\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", google_api_key=GOOGLE_API_KEY, temperature=0.5) \n",
    "    chain = create_tool_calling_agent(llm, tools, prompt_gemini_format) \n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=active_memory, \n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "input_1 = \"Envie uma mensagem para mim dizendo oi?.\"\n",
    "resposta_1 = agent_executor.invoke({\"input\": input_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool # Adicionado o import que faltava na sua Ãºltima versÃ£o\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "import redis\n",
    "import os\n",
    "import requests \n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "user_number = \"5541999999999\" \n",
    "\n",
    "def get_memory_for_user(session_id_key: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\", chat_memory=chat_memory)\n",
    "\n",
    "user_id = \"John_Santos_Test_Session\"\n",
    "memoria = get_memory_for_user(user_id)\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada para o WhatsApp do usuÃ¡rio.\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    try:\n",
    "        resposta = requests.post(url, headers=headers, json=payload)\n",
    "        resposta.raise_for_status()\n",
    "        return resposta.json()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao enviar mensagem: {e}\"\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memÃ³ria da conversa atual  do Redis.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(REDIS_URL)\n",
    "        num_deleted = r.delete(user_id)\n",
    "        return f\"MemÃ³ria para a sessÃ£o '{user_id}' foi excluÃ­da com sucesso.\" if num_deleted > 0 else f\"Nenhuma memÃ³ria encontrada para a sessÃ£o '{user_id}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao tentar excluir memÃ³ria: {e}\"\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"VocÃª Ã© Morpheus, um assistente disruptivo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.0)\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=memoria,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True, \n",
    "    handle_parsing_errors=True  \n",
    ")\n",
    "\n",
    "primeira_input = \"Envie uma mensagem para mim dizendo que bonito\"\n",
    "resposta1 = agent_executor.invoke({\"input\": primeira_input})\n",
    "print(resposta1.get(\"output\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada para o WhatsApp do usuÃ¡rio.\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    try:\n",
    "        resposta = requests.post(url, headers=headers, json=payload)\n",
    "        resposta.raise_for_status()\n",
    "        return resposta.json()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao enviar mensagem: {e}\"\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memÃ³ria da conversa atual  do Redis.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(REDIS_URL)\n",
    "        num_deleted = r.delete(user_id)\n",
    "        return f\"MemÃ³ria para a sessÃ£o '{user_id}' foi excluÃ­da com sucesso.\" if num_deleted > 0 else f\"Nenhuma memÃ³ria encontrada para a sessÃ£o '{user_id}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao tentar excluir memÃ³ria: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "\n",
    "query = \"eNVIE UMA MENSAGEM  de whatsapp mandando um oi\"\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(ai_msg)\n",
    "\n",
    "messages.append(ai_msg)\n",
    "\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"enviar_msg\": enviar_msg, \"excluir_memoria\": excluir_memoria}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages\n",
    "\n",
    "llm_with_tools.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "model = \"google/gemini-2.5-pro-preview\"\n",
    "\n",
    "\n",
    "def invoke(message):\n",
    "    response = requests.post(\n",
    "\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": \"Bearer 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\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"model\": model, \n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": message\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                    \"url\": \"\"\n",
    "                    }\n",
    "                }]\n",
    "            }],\n",
    "        })\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "invoke(\"quem Ã© julio cesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osan/Documentos/Dev/Morpheus/venv/lib/python3.13/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='primeiro numero mencionado pelo usuario'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `soma` with `{'num1': 22, 'num2': 55}`\n",
      "responded: <think>\n",
      "Okay, the user mentioned \"numero 22 a outra parte da organizaÃ§Ã£o se chamava 55\" which translates to \"number 22, the other part of the organization was called 55.\" They want me to use the tool instead of calculating manually.\n",
      "\n",
      "First, I need to check if the tool is available. The tool provided is the \"soma\" function, which adds two numbers. The user has been asking for the sum of 22 and 55 multiple times, but previously, I couldn't use the tool because the parameters weren't correctly set. Wait, looking back, the function requires \"num1\" and \"num2\" as integers. In the initial tool definition, the required parameter was only \"num2\", but that's probably a mistake. The user is asking for the sum of 22 and 55, so I should call the \"soma\" function with num1=22 and num2=55. \n",
      "\n",
      "Wait, the tool's parameters might have an error. The original tool definition says \"required\": [\"num2\"], but that doesn't make sense because you need both numbers to add. Maybe it's a typo, and both should be required. Assuming the tool can take both numbers, I'll proceed to call the function with num1=22 and num2=55. The user wants the tool to be used, so even if there was a previous error, I should correct it and use the function properly. The result should be 77, and I'll present it as such.\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m----------------manguatirica---------------\n",
      "\u001b[36;1m\u001b[1;3m77\u001b[0m\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user has been repeatedly asking to sum 22 and 55 using the \"soma\" tool, but previously I couldn't use the tool correctly. Now, after the tool response of 77, I need to confirm the result and offer further help. Let me make sure to acknowledge the tool's output and keep the response friendly.\n",
      "</think>\n",
      "\n",
      "A soma dos nÃºmeros 22 e 55 usando a ferramenta Ã© **77**.  \n",
      "Se precisar de mais ajuda, Ã© sÃ³ pedir! ðŸ˜Š\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "<think>\n",
      "Okay, the user has been repeatedly asking to sum 22 and 55 using the \"soma\" tool, but previously I couldn't use the tool correctly. Now, after the tool response of 77, I need to confirm the result and offer further help. Let me make sure to acknowledge the tool's output and keep the response friendly.\n",
      "</think>\n",
      "\n",
      "A soma dos nÃºmeros 22 e 55 usando a ferramenta Ã© **77**.  \n",
      "Se precisar de mais ajuda, Ã© sÃ³ pedir! ðŸ˜Š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osan/Documentos/Dev/Morpheus/venv/lib/python3.13/site-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from tools import soma\n",
    "import os\n",
    "\n",
    "REDIS_URL = \"redis://default:QR02Eq2QKPlH6pJMN4wUo3XuEKUfqrAz@redis-16516.crce196.sa-east-1-2.ec2.redns.redis-cloud.com:16516\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "def get_memory_for_user(session_id):\n",
    "    history = RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key='memoria', chat_memory=history)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"VocÃª Ã© um assistente Ãºtil que usa as tools conforme necessidade.\",\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name='memoria'),\n",
    "    ('human', '{input}'),\n",
    "    MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "])\n",
    "\n",
    "#parcial_prompt = prompt.partial(\n",
    "#    input_language=\"portugues\",\n",
    "#    output_language=\"japones\"\n",
    "#)\n",
    "\n",
    "tools = [soma]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "active_memory = get_memory_for_user('session_id_placeholder') \n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=active_memory, \n",
    "    return_intermediate_steps=True,\n",
    "    tools=tools,  \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    'input': 'numero 22 a outra parte da organizaÃ§Ã£o se chamava 55',\n",
    "})\n",
    "\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
