{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())  \n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")\n",
    "\n",
    "response = llm.invoke(\"Olá Gemini\") \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import redis\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "#-------------------------------------------------Functions---------------------------------------------------------------\n",
    "\n",
    "def obter_hora_e_data_atual():\n",
    "    agora = datetime.now()\n",
    "    return agora.strftime(\"%d-%m-%Y - %H:%M:%S\")\n",
    "\n",
    "def get_memory_for_user(whatsapp):\n",
    "    memory = RedisChatMessageHistory(\n",
    "        session_id=whatsapp, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"memory\", chat_memory=memory)\n",
    "\n",
    "def get_memory_for_user_gemini(session_id_key: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\", chat_memory=chat_memory)\n",
    "\n",
    "\n",
    "#------------------------------------------------Webhooks-------------------------------------------------------------------\n",
    "\n",
    "user_id = \"John Santos\"\n",
    "user_number = \"5541996143338\"\n",
    "data_atual = obter_hora_e_data_atual()\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------Tools----------------------------------------------------------\n",
    "    \n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    responsta = requests.post(url, headers=headers, json=payload)\n",
    "    responsta.raise_for_status()\n",
    "    return responsta.json()\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memoria da conversa quando solicitado\"\"\"\n",
    "    r = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "    session_prefix = f\"message_store:{user_id}\"\n",
    "    chaves = r.keys(session_prefix + \"*\")\n",
    "    for chave in chaves:\n",
    "        r.delete(chave)\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "memoria_gemini = get_memory_for_user_gemini(user_id)\n",
    "memoria = get_memory_for_user(user_id)\n",
    "\n",
    "#--------------------------------------------------Tools----------------------------------------------------------\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"\"\"\n",
    "        Você é Morpheus, um assistente disruptivo. A data atual é {data_atual}.\n",
    "        Não use asteriscos, listas devem começar com '-'.\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "prompt_gemini = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é Morpheus, um assistente disruptivo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=api_key, temperature=0.0)\n",
    "    tools_json = [convert_to_openai_function(t) for t in tools]\n",
    "    pass_through = RunnablePassthrough.assign(agent_scratchpad=lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"]))\n",
    "    chain = pass_through | prompt | llm.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=memoria,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True\n",
    "    )\n",
    "\n",
    "elif LLM_PROVIDER == \"gemini\":\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro-preview-05-06\", google_api_key=GOOGLE_API_KEY, temperature=0.0)\n",
    "    chain = create_tool_calling_agent(llm, tools, prompt_gemini)\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=memoria_gemini,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True, \n",
    "        handle_parsing_errors=True  \n",
    "    )\n",
    "\n",
    "\n",
    "resposta = agent_executor.invoke({\"input\": \"qual foi a ultima mensagem que lhe enviei?\"})\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtional selection of Gpt an Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `enviar_msg` with `{'texto': 'oi'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'key': {'remoteJid': '554196143338@s.whatsapp.net', 'fromMe': True, 'id': '3EB0B2AA82D8DEFF0F5F5780A3C75C13A03B1B22'}, 'pushName': '', 'status': 'PENDING', 'message': {'conversation': 'oi'}, 'contextInfo': None, 'messageType': 'conversation', 'messageTimestamp': 1748007964, 'instanceId': '40d11540-3974-449a-967f-2b7a28d621b3', 'source': 'unknown'}\u001b[0m\u001b[32;1m\u001b[1;3mEnviei uma mensagem para você dizendo \"oi\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# ------------------------------------------------- Functions ---------------------------------------------------------------\n",
    "\n",
    "def obter_hora_e_data_atual():\n",
    "    agora = datetime.now()\n",
    "    return agora.strftime(\"%d-%m-%Y - %H:%M:%S\")\n",
    "\n",
    "def get_memory(session_id_key: str, memory_key_name: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=memory_key_name, chat_memory=chat_memory, output_key='output')\n",
    "\n",
    "# ------------------------------------------------ Webhooks -------------------------------------------------------------------\n",
    "\n",
    "user_id = \"John_Santos\"\n",
    "user_number = \"5541996143338\"\n",
    "data_atual = obter_hora_e_data_atual()\n",
    "\n",
    "# -------------------------------------------------- Tools ----------------------------------------------------------\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": user_number,\n",
    "        \"text\": texto,\n",
    "        \"options\": {\"delay\": 800}\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Erro na requisição para Evolution API: {e}\"\n",
    "    \n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memoria da conversa atual (session_id) do Redis.\"\"\"\n",
    "    chat_memory_to_clear = RedisChatMessageHistory(\n",
    "        session_id=user_id,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    chat_memory_to_clear.clear()\n",
    "    return f\"Memória para a sessão '{user_id}' foi excluída com sucesso.\"\n",
    "\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "\n",
    "# -------------------------------------------------- Prompts ----------------------------------------------------------\n",
    "\n",
    "\n",
    "prompt_openai_format = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"Você é Morpheus (OpenAI). Data: {data_atual}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"memory\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_gemini_format = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        f\"Você é Morpheus (Gemini), um assistente que mantém o contexto da conversa.\"\n",
    "        \"**Instruções para o uso de ferramentas:**\\n\"\n",
    "        \"- **Para enviar uma mensagem**: Se o usuário solicitar 'enviar uma mensagem' ou 'enviar um ', utilize a ferramenta\\n\"\n",
    "        \"- **Para excluir a memória**: Se o usuário pedir para 'excluir a memória', 'limpar a conversa' ou frases similares, utilize a ferramenta `excluir_memoria`.\\n\\n\"\n",
    "        \"Responda de forma útil e concisa, usando as ferramentas quando apropriado.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "\n",
    "if LLM_PROVIDER == \"openai\":\n",
    "\n",
    "    active_memory = get_memory(user_id, \"memory\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=OPENAI_API_KEY, temperature=0.0)\n",
    "    tools_json = [convert_to_openai_function(t) for t in tools]\n",
    "    chain = RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_function_messages(x[\"intermediate_steps\"])) | prompt_openai_format | llm.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=active_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "\n",
    "elif LLM_PROVIDER == \"gemini\":\n",
    "\n",
    "    active_memory = get_memory(user_id, \"chat_history\")\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\", google_api_key=GOOGLE_API_KEY, temperature=0.5) \n",
    "    chain = create_tool_calling_agent(llm, tools, prompt_gemini_format) \n",
    "\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=chain,\n",
    "        memory=active_memory, \n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "input_1 = \"Envie uma mensagem para mim dizendo oi?.\"\n",
    "resposta_1 = agent_executor.invoke({\"input\": input_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool # Adicionado o import que faltava na sua última versão\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "import redis\n",
    "import os\n",
    "import requests \n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")\n",
    "EVOLUTION_URL = os.getenv(\"EVOLUTION_URL\")\n",
    "INSTANCE_ID = os.getenv(\"EVOLUTION_INSTANCE\")\n",
    "EVOLUTION_TOKEN = os.getenv(\"EVOLUTION_APIKEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "user_number = \"5541999999999\" \n",
    "\n",
    "def get_memory_for_user(session_id_key: str):\n",
    "    chat_memory = RedisChatMessageHistory(\n",
    "        session_id=session_id_key, \n",
    "        url=REDIS_URL)\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\", chat_memory=chat_memory)\n",
    "\n",
    "user_id = \"John_Santos_Test_Session\"\n",
    "memoria = get_memory_for_user(user_id)\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada para o WhatsApp do usuário.\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    try:\n",
    "        resposta = requests.post(url, headers=headers, json=payload)\n",
    "        resposta.raise_for_status()\n",
    "        return resposta.json()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao enviar mensagem: {e}\"\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memória da conversa atual  do Redis.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(REDIS_URL)\n",
    "        num_deleted = r.delete(user_id)\n",
    "        return f\"Memória para a sessão '{user_id}' foi excluída com sucesso.\" if num_deleted > 0 else f\"Nenhuma memória encontrada para a sessão '{user_id}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao tentar excluir memória: {e}\"\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é Morpheus, um assistente disruptivo.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") \n",
    "])\n",
    "\n",
    "LLM_PROVIDER = \"gemini\" \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.0)\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=memoria,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True, \n",
    "    handle_parsing_errors=True  \n",
    ")\n",
    "\n",
    "primeira_input = \"Envie uma mensagem para mim dizendo que bonito\"\n",
    "resposta1 = agent_executor.invoke({\"input\": primeira_input})\n",
    "print(resposta1.get(\"output\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "\n",
    "class Enviar_Evolution(BaseModel):\n",
    "    texto: str = Field(description=\"Mensagem a ser enviada para o WhatsApp do usuário.\")\n",
    "\n",
    "@tool(args_schema=Enviar_Evolution)\n",
    "def enviar_msg(texto: str):\n",
    "    \"\"\"Envia uma mensagem de whatsapp ao usuario\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/message/sendText/{INSTANCE_ID}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\", \n",
    "        \"apikey\": EVOLUTION_TOKEN\n",
    "    }\n",
    "    payload = {\n",
    "        \"number\": \"5541996143338\", \n",
    "        \"text\": texto, \n",
    "        \"delay\": 800\n",
    "    }\n",
    "    try:\n",
    "        resposta = requests.post(url, headers=headers, json=payload)\n",
    "        resposta.raise_for_status()\n",
    "        return resposta.json()\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao enviar mensagem: {e}\"\n",
    "\n",
    "@tool\n",
    "def excluir_memoria():\n",
    "    \"\"\"Exclui a memória da conversa atual  do Redis.\"\"\"\n",
    "    try:\n",
    "        r = redis.from_url(REDIS_URL)\n",
    "        num_deleted = r.delete(user_id)\n",
    "        return f\"Memória para a sessão '{user_id}' foi excluída com sucesso.\" if num_deleted > 0 else f\"Nenhuma memória encontrada para a sessão '{user_id}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao tentar excluir memória: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "tools = [enviar_msg, excluir_memoria]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "\n",
    "query = \"eNVIE UMA MENSAGEM  de whatsapp mandando um oi\"\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(ai_msg)\n",
    "\n",
    "messages.append(ai_msg)\n",
    "\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"enviar_msg\": enviar_msg, \"excluir_memoria\": excluir_memoria}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages\n",
    "\n",
    "llm_with_tools.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "model = \"google/gemini-2.5-pro-preview\"\n",
    "\n",
    "\n",
    "def invoke(message):\n",
    "    response = requests.post(\n",
    "\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": \"Bearer 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\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"model\": model, \n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": message\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                    \"url\": \"\"\n",
    "                    }\n",
    "                }]\n",
    "            }],\n",
    "        })\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "invoke(\"quem é julio cesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osan/Documentos/Dev/Morpheus/venv/lib/python3.13/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='primeiro numero mencionado pelo usuario'),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `soma` with `{'num1': 22, 'num2': 55}`\n",
      "responded: <think>\n",
      "Okay, the user mentioned \"numero 22 a outra parte da organização se chamava 55\" which translates to \"number 22, the other part of the organization was called 55.\" They want me to use the tool instead of calculating manually.\n",
      "\n",
      "First, I need to check if the tool is available. The tool provided is the \"soma\" function, which adds two numbers. The user has been asking for the sum of 22 and 55 multiple times, but previously, I couldn't use the tool because the parameters weren't correctly set. Wait, looking back, the function requires \"num1\" and \"num2\" as integers. In the initial tool definition, the required parameter was only \"num2\", but that's probably a mistake. The user is asking for the sum of 22 and 55, so I should call the \"soma\" function with num1=22 and num2=55. \n",
      "\n",
      "Wait, the tool's parameters might have an error. The original tool definition says \"required\": [\"num2\"], but that doesn't make sense because you need both numbers to add. Maybe it's a typo, and both should be required. Assuming the tool can take both numbers, I'll proceed to call the function with num1=22 and num2=55. The user wants the tool to be used, so even if there was a previous error, I should correct it and use the function properly. The result should be 77, and I'll present it as such.\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m----------------manguatirica---------------\n",
      "\u001b[36;1m\u001b[1;3m77\u001b[0m\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, the user has been repeatedly asking to sum 22 and 55 using the \"soma\" tool, but previously I couldn't use the tool correctly. Now, after the tool response of 77, I need to confirm the result and offer further help. Let me make sure to acknowledge the tool's output and keep the response friendly.\n",
      "</think>\n",
      "\n",
      "A soma dos números 22 e 55 usando a ferramenta é **77**.  \n",
      "Se precisar de mais ajuda, é só pedir! 😊\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "<think>\n",
      "Okay, the user has been repeatedly asking to sum 22 and 55 using the \"soma\" tool, but previously I couldn't use the tool correctly. Now, after the tool response of 77, I need to confirm the result and offer further help. Let me make sure to acknowledge the tool's output and keep the response friendly.\n",
      "</think>\n",
      "\n",
      "A soma dos números 22 e 55 usando a ferramenta é **77**.  \n",
      "Se precisar de mais ajuda, é só pedir! 😊\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osan/Documentos/Dev/Morpheus/venv/lib/python3.13/site-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from tools import soma\n",
    "import os\n",
    "\n",
    "REDIS_URL = \"redis://default:QR02Eq2QKPlH6pJMN4wUo3XuEKUfqrAz@redis-16516.crce196.sa-east-1-2.ec2.redns.redis-cloud.com:16516\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "def get_memory_for_user(session_id):\n",
    "    history = RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=REDIS_URL\n",
    "    )\n",
    "    return ConversationBufferMemory(return_messages=True, memory_key='memoria', chat_memory=history)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Você é um assistente útil que usa as tools conforme necessidade.\",\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name='memoria'),\n",
    "    ('human', '{input}'),\n",
    "    MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "])\n",
    "\n",
    "#parcial_prompt = prompt.partial(\n",
    "#    input_language=\"portugues\",\n",
    "#    output_language=\"japones\"\n",
    "#)\n",
    "\n",
    "tools = [soma]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "active_memory = get_memory_for_user('session_id_placeholder') \n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    memory=active_memory, \n",
    "    return_intermediate_steps=True,\n",
    "    tools=tools,  \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    'input': 'numero 22 a outra parte da organização se chamava 55',\n",
    "})\n",
    "\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
